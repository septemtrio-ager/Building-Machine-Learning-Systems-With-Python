{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# クラスタリング：関連のある文書を見つける\n",
    "\n",
    "訓練データに各データに属するクラスがラベル付されており、その訓練データからモデルを学習することができる。これは「教師あり学習」と呼ばれている。今回はそういったラベルが付いていないデータに対して分類モデルを構築する方法を考えていくことになる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここではQ&Aサイトを運営する場面を想定して、現在見ているページの内容と関連する情報を提示することを考える。この問題に対するアプローチとして、「ある投稿された文書に対して、それ以外の文書との類似度をすべて算出する」事になるでしょう。その後「上位N個の文章に関するリンクをページ下部に表示する」といったことを行えばいいのではないだろうか。しかしながらこのアプローチでは、ページ数が増えてしまうと計算が追いつかなくなってしまう。そういったことを踏まえて、関連している文書を**素早く**見つける方法を見つけていく。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これは**クラスタリング（clustering）**を用いることで解決できる。クラスタとはデータ集合の部分集合であり、クラスタリングとは似ているもの同士を同じクラスタに分類する手法を指している。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文書の関連性を計測する\n",
    "\n",
    "機械学習においては、テキストデータだけでは分析が出来ないため、データを数字に変換することが必要になってくる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### やってはいけないこと\n",
    "\n",
    "テキストデータの類似度を求めるために、レーベンシュタイン距離（Levenshtein distance）を利用することができる。レーベンシュタイン距離は**編集距離**と呼ばれることもある。２つの単語があった場合、一方の単語をもう一方の単語と同じアルファベットの並びにするために、編集を行う最小回数をいう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "たとえば「machine」と「mchiene」という単語があったとすると、この場合編集距離は*2*になる。すなわち、\n",
    "\n",
    "1. mの後にaを挿入する\n",
    "2. 最初のeを削除する\n",
    "\n",
    "こういったアルゴリズムは計算コストがとても高くなる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文章内の単語を最小単位として扱うことにして、文章全体の編集距離を考えてみる。ここでは文書1*「How to format my hard disk」*と文書2*「Hard disk format problems」*という文章の編集距離を考えてみる。\n",
    "\n",
    "1. howを削除する\n",
    "2. toを削除する\n",
    "3. formatを削除する\n",
    "4. myを削除する\n",
    "5. formatを追加する\n",
    "6. problemsを追加する\n",
    "\n",
    "この場合変数距離は*6*とすることができる。これらは単語の編集距離よりかはコストを削減できていそうだが、時間のオーダは同じであるため、問題が残ってしまう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### どうやるべきか\n",
    "\n",
    "編集距離よりもロバストな手法として、**bag-of-words**と呼ばれるアプローチがある。この手法は単語の出現回数を特徴量として利用する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文書1と文書2について、単語の出現回数は下のようになる。\n",
    "\n",
    "|単語      |文書1での出現回数|文書2での出現回数|\n",
    "|:---------|:---------------:|:---------------:|\n",
    "| disk     | 1               | 1               |\n",
    "| format   | 1               | 1               |\n",
    "| now      | 1               | 0               |\n",
    "| hard     | 1               | 1               |\n",
    "| my       | 1               | 0               |\n",
    "| problems | 0               | 1               |\n",
    "| to       | 1               | 0               |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文書1と文書2の列をベクトルとして扱うことができる。このようにベクトル化することによってデータ・セット中のすべての文書館でユークリッド距離を計算することができ、最近傍点を見つけることができる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "クラスタリングの流れは次のようになる。\n",
    "\n",
    "1. 各文書から特徴量を抽出し、特徴ベクトルの形で保存する。\n",
    "2. 特徴ベクトルに対して、クラスタリングを行う。\n",
    "3. 投稿された質問文書に対して、クラス決定する。\n",
    "4. このクラスタに属する文書を他にいくつか集める。これで多様性を増やせる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前処理：共通する単語の出現回数を類似度として計測する\n",
    "\n",
    "bag-of-wordは高速に処理することができるが、いくつか問題点も存在する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### テキストデータをbag-of-wordに変換する\n",
    "\n",
    "ここではScikitのCountVectorizerを利用して単語の出現回数を数え、それをベクトルで表記できるようにする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*min_df*というパラメータは頻繁に使われ得ていない単語をCountVectorizerが無視するときに利用する。整数を渡せばその数より出現頻度の少ない単語は無視される。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成したインスタンスをprintしてみると、Scikitがデフォルトのパラメータとしてどういった値を設定しているのかを確認することができる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*analyzer=word*と指定されている通り、これは単語レベルで出現回数が数えられていることを示している。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['disk', 'format', 'hard', 'how', 'my', 'ploblem', 'to']"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contents = ['How to format my hard disk', 'Hard disk format ploblem']\n",
    "X = vectorizer.fit_transform(contents)\n",
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*vectorizer*は7つの単語を、出現回数を数えるべき単語としてトークンとしている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X.toarray().transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これは単語の出現回数の表と同じ結果になっていることが分かる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 単語を数える\n",
    "\n",
    "ここでは*sample_documents*ディレクトリ下にある文書をデータセットとして分析を行ってみる。 [03.txtについて補足](supplemental.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01.txt : This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "02.txt : Imaging databases provide storage capabilities.\n",
      "03.txt : Most imaging databases save images permanently.\n",
      "04.txt : Imaging databases store data.\n",
      "05.txt : Imaging databases store data. Imaging databases store data. Imaging databases store data.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "SAMPLE_DOC_DIR = \"sample_documents\"\n",
    "posts = [open(os.path.join(SAMPLE_DOC_DIR, f)).read() for f in sorted(os.listdir(SAMPLE_DOC_DIR))]\n",
    "\n",
    "for num, post in enumerate(posts):\n",
    "    print(\"0%d.txt : %s\" % (num + 1, post))\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*vectorizer*には対象となる文書データをすべて知らせる必要がある。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#samples: 5, #features: 25\n"
     ]
    }
   ],
   "source": [
    "x_train = vectorizer.fit_transform(posts)\n",
    "num_samples, num_features = x_train.shape\n",
    "print(\"#samples: %d, #features: %d\" % (num_samples, num_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5つの文章から25個の単語が存在することを示している。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['about', 'actually', 'capabilities', 'contains', 'data', 'databases', 'images', 'imaging', 'interesting', 'is', 'it', 'learning', 'machine', 'most', 'much', 'not', 'permanently', 'post', 'provide', 'save', 'storage', 'store', 'stuff', 'this', 'toy']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "新しい文書について、次のようにベクトル化することができる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_post = \"imaging databases\"\n",
    "new_post_vec = vectorizer.transform([new_post])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**transform()**によって返されるベクトルは疎なベクトルである。そのため、疎ベクトルに適したデータ構造が結果として返される。ほとんどの要素が0であるため、すべての単語の出現回数をまとめて表記することは行えない。代わりに出現した単語のみの情報だけをデータに格納する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 5)\t1\n",
      "  (0, 7)\t1\n"
     ]
    }
   ],
   "source": [
    "print(new_post_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**toarray()**を使うことで、特徴ベクトルのすべての要素を表示することができる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]] , type=<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(\"%s , type=%s\" % (new_post_vec.toarray(), type(new_post_vec.toarray())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "類似度を算出するために、新しい文書と他の既存の文書の間でユークリッド距離を計算することにする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "\n",
    "def dist_raw(v1, v2):\n",
    "    delta = v1 - v2\n",
    "    return sp.linalg.norm(delta.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここで**norm()**はユークリッドノルム（ユークリッド距離）を計算している。定義したdist_raw()を用いることで、他のすべての文書に対して類似度を計算することができる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Post 0 with dist=4.00: This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "=== Post 1 with dist=1.73: Imaging databases provide storage capabilities.\n",
      "=== Post 2 with dist=2.00: Most imaging databases save images permanently.\n",
      "=== Post 3 with dist=1.41: Imaging databases store data.\n",
      "=== Post 4 with dist=5.10: Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      "\n",
      "[Result]\n",
      "Best post is 3 with dist=1.41\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# 引数にベクトルの距離を計算する関数を渡す\n",
    "def print_doc_dists(dist_func):\n",
    "    best_doc = None\n",
    "    best_dist = sys.maxsize\n",
    "    best_i = None\n",
    "\n",
    "    for i in range(0, num_samples):\n",
    "        post = posts[i]\n",
    "    \n",
    "        if post == new_post:\n",
    "            continue\n",
    "\n",
    "        post_vec = x_train.getrow(i)\n",
    "\n",
    "        d = dist_func(post_vec, new_post_vec)\n",
    "        print(\"=== Post %i with dist=%.2f: %s\" % (i, d, post))\n",
    "\n",
    "        if d < best_dist:\n",
    "            best_dist = d\n",
    "            best_i = i\n",
    "\n",
    "    print(\"\\n[Result]\\nBest post is %i with dist=%.2f\" % (best_i, best_dist))\n",
    "\n",
    "# dist_raw()を使って距離を計算するように\n",
    "print_doc_dists(dist_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "新しい文書*\"imaging databases\"*と最も似ていない文書は*文書0*である結果が出た。たしかに、これら2つの文書間には共通する単語は存在しない。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "また、新しい文書は文書1とよく似ていることが分かる。しかしながら最も類似しているものは文書3である。文書1と文書3はともに新しい文書の単語を含むが、文書1のほうが1単語多い文章から構成されている。よって文書3のほうが類似度が高くなる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "新しい文書に対しての類似度は、その2つの文書で同じであるべきだ。しかしながら結果は異なっている。ここで文書3と文書4の特徴ベクトルを出力してみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]]\n",
      "[[0 0 0 0 3 3 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train.getrow(3).toarray())\n",
    "print(x_train.getrow(4).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "単語の出現回数だけを特徴量として用いているようである。これは正規化する必要がある。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 単語の出現回数ベクトルを正規化する\n",
    "\n",
    "単語の出現回数からなるベクトルではなく、正規化したベクトルを返すようにdist_raw()を拡張してみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dist_norm(v1, v2):\n",
    "    v1_normalized = v1 / sp.linalg.norm(v1.toarray())\n",
    "    v2_normalized = v2 / sp.linalg.norm(v2.toarray())\n",
    "\n",
    "    delta = v1_normalized - v2_normalized\n",
    "    \n",
    "    return sp.linalg.norm(delta.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このdist_norm()を使って、類似度を計算してみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Post 0 with dist=1.41: This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "=== Post 1 with dist=0.86: Imaging databases provide storage capabilities.\n",
      "=== Post 2 with dist=0.92: Most imaging databases save images permanently.\n",
      "=== Post 3 with dist=0.77: Imaging databases store data.\n",
      "=== Post 4 with dist=0.77: Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      "\n",
      "[Result]\n",
      "Best post is 3 with dist=0.77\n"
     ]
    }
   ],
   "source": [
    "print_doc_dists(dist_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回の結果は文書3と文書4で同じ類似度が出ていることが分かる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 重要度の低い単語を取り除く\n",
    "\n",
    "今度は新しい文書*「imaging databases」*と文書2を比較してみることにする。新しい文書には、*most*、*safe*、*images*、*permanently*が含まれていない。こういった単語は文書内では重要度がそれぞれ異なっている。*most*は分野に関係なく様々な文章に登場してくる。こういった単語を**ストップワード（stop word）**として、処理の対象外とすべきである。ストップワードとは頻繁に使われる単語、文書の分類に貢献しない単語のことである。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizerでは、簡単にストップワードを登録することができる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df=1, stop_words='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここではenglishと指定することで、318個の単語をストップワードとして登録できる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'across', 'after', 'afterwards', 'again', 'against', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'amoungst']\n"
     ]
    }
   ],
   "source": [
    "print(sorted(vectorizer.get_stop_words())[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ストップワードを利用すると、単語リストは全部で7つ減る。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['actually', 'capabilities', 'contains', 'data', 'databases', 'images', 'imaging', 'interesting', 'learning', 'machine', 'permanently', 'post', 'provide', 'save', 'storage', 'store', 'stuff', 'toy']\n"
     ]
    }
   ],
   "source": [
    "x_train = vectorizer.fit_transform(posts)\n",
    "num_samples, num_features = x_train.shape\n",
    "print(vectorizer.get_feature_names())\n",
    "new_post_vec = vectorizer.transform([new_post])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "またストップワードを用いることで、類似度は次のようになる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Post 0 with dist=1.41: This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "=== Post 1 with dist=0.86: Imaging databases provide storage capabilities.\n",
      "=== Post 2 with dist=0.86: Most imaging databases save images permanently.\n",
      "=== Post 3 with dist=0.77: Imaging databases store data.\n",
      "=== Post 4 with dist=0.77: Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      "\n",
      "[Result]\n",
      "Best post is 3 with dist=0.77\n"
     ]
    }
   ],
   "source": [
    "print_doc_dists(dist_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文章1と文書2は同じ類似度になっていることが分かる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ステミング（stemming）\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
