{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# クラスタリング：関連のある文書を見つける\n",
    "\n",
    "訓練データに各データに属するクラスがラベル付されており、その訓練データからモデルを学習することができる。これは「教師あり学習」と呼ばれている。今回はそういったラベルが付いていないデータに対して分類モデルを構築する方法を考えていくことになる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここではQ&Aサイトを運営する場面を想定して、現在見ているページの内容と関連する情報を提示することを考える。この問題に対するアプローチとして、「ある投稿された文書に対して、それ以外の文書との類似度をすべて算出する」事になるでしょう。その後「上位N個の文章に関するリンクをページ下部に表示する」といったことを行えばいいのではないだろうか。しかしながらこのアプローチでは、ページ数が増えてしまうと計算が追いつかなくなってしまう。そういったことを踏まえて、関連している文書を**素早く**見つける方法を見つけていく。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これは**クラスタリング（clustering）**を用いることで解決できる。クラスタとはデータ集合の部分集合であり、クラスタリングとは似ているもの同士を同じクラスタに分類する手法を指している。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文書の関連性を計測する\n",
    "\n",
    "機械学習においては、テキストデータだけでは分析が出来ないため、データを数字に変換することが必要になってくる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### やってはいけないこと\n",
    "\n",
    "テキストデータの類似度を求めるために、レーベンシュタイン距離（Levenshtein distance）を利用することができる。レーベンシュタイン距離は**編集距離**と呼ばれることもある。２つの単語があった場合、一方の単語をもう一方の単語と同じアルファベットの並びにするために、編集を行う最小回数をいう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "たとえば「machine」と「mchiene」という単語があったとすると、この場合編集距離は*2*になる。すなわち、\n",
    "\n",
    "1. mの後にaを挿入する\n",
    "2. 最初のeを削除する\n",
    "\n",
    "こういったアルゴリズムは計算コストがとても高くなる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文章内の単語を最小単位として扱うことにして、文章全体の編集距離を考えてみる。ここでは文書1*「How to format my hard disk」*と文書2*「Hard disk format problems」*という文章の編集距離を考えてみる。\n",
    "\n",
    "1. howを削除する\n",
    "2. toを削除する\n",
    "3. formatを削除する\n",
    "4. myを削除する\n",
    "5. formatを追加する\n",
    "6. problemsを追加する\n",
    "\n",
    "この場合変数距離は*6*とすることができる。これらは単語の編集距離よりかはコストを削減できていそうだが、時間のオーダは同じであるため、問題が残ってしまう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### どうやるべきか\n",
    "\n",
    "編集距離よりもロバストな手法として、**bag-of-words**と呼ばれるアプローチがある。この手法は単語の出現回数を特徴量として利用する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文書1と文書2について、単語の出現回数は下のようになる。\n",
    "\n",
    "|単語      |文書1での出現回数|文書2での出現回数|\n",
    "|:---------|:---------------:|:---------------:|\n",
    "| disk     | 1               | 1               |\n",
    "| format   | 1               | 1               |\n",
    "| now      | 1               | 0               |\n",
    "| hard     | 1               | 1               |\n",
    "| my       | 1               | 0               |\n",
    "| problems | 0               | 1               |\n",
    "| to       | 1               | 0               |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文書1と文書2の列をベクトルとして扱うことができる。このようにベクトル化することによってデータ・セット中のすべての文書館でユークリッド距離を計算することができ、最近傍点を見つけることができる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "クラスタリングの流れは次のようになる。\n",
    "\n",
    "1. 各文書から特徴量を抽出し、特徴ベクトルの形で保存する。\n",
    "2. 特徴ベクトルに対して、クラスタリングを行う。\n",
    "3. 投稿された質問文書に対して、クラス決定する。\n",
    "4. このクラスタに属する文書を他にいくつか集める。これで多様性を増やせる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前処理：共通する単語の出現回数を類似度として計測する\n",
    "\n",
    "bag-of-wordは高速に処理することができるが、いくつか問題点も存在する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### テキストデータをbag-of-wordに変換する\n",
    "\n",
    "ここではScikitのCountVectorizerを利用して単語の出現回数を数え、それをベクトルで表記できるようにする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*min_df*というパラメータは頻繁に使われ得ていない単語をCountVectorizerが無視するときに利用する。整数を渡せばその数より出現頻度の少ない単語は無視される。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成したインスタンスをprintしてみると、Scikitがデフォルトのパラメータとしてどういった値を設定しているのかを確認することができる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*analyzer=word*と指定されている通り、これは単語レベルで出現回数が数えられていることを示している。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['disk', 'format', 'hard', 'how', 'my', 'ploblem', 'to']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contents = ['How to format my hard disk', 'Hard disk format ploblem']\n",
    "X = vectorizer.fit_transform(contents)\n",
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*vectorizer*は7つの単語を、出現回数を数えるべき単語としてトークンとしている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X.toarray().transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これは単語の出現回数の表と同じ結果になっていることが分かる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 単語を数える\n",
    "\n",
    "ここでは*sample_documents*ディレクトリ下にある文書をデータセットとして分析を行ってみる。 [03.txtについて補足](supplemental.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01.txt : This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "02.txt : Imaging databases provide storage capabilities.\n",
      "03.txt : Most imaging databases save images permanently.\n",
      "04.txt : Imaging databases store data.\n",
      "05.txt : Imaging databases store data. Imaging databases store data. Imaging databases store data.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "SAMPLE_DOC_DIR = \"sample_documents\"\n",
    "posts = [open(os.path.join(SAMPLE_DOC_DIR, f)).read() for f in sorted(os.listdir(SAMPLE_DOC_DIR))]\n",
    "\n",
    "for num, post in enumerate(posts):\n",
    "    print(\"0%d.txt : %s\" % (num + 1, post))\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*vectorizer*には対象となる文書データをすべて知らせる必要がある。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#samples: 5, #features: 25\n"
     ]
    }
   ],
   "source": [
    "x_train = vectorizer.fit_transform(posts)\n",
    "num_samples, num_features = x_train.shape\n",
    "print(\"#samples: %d, #features: %d\" % (num_samples, num_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5つの文章から25個の単語が存在することを示している。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['about', 'actually', 'capabilities', 'contains', 'data', 'databases', 'images', 'imaging', 'interesting', 'is', 'it', 'learning', 'machine', 'most', 'much', 'not', 'permanently', 'post', 'provide', 'save', 'storage', 'store', 'stuff', 'this', 'toy']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "新しい文書について、次のようにベクトル化することができる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_post = \"imaging databases\"\n",
    "new_post_vec = vectorizer.transform([new_post])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**transform()**によって返されるベクトルは疎なベクトルである。そのため、疎ベクトルに適したデータ構造が結果として返される。ほとんどの要素が0であるため、すべての単語の出現回数をまとめて表記することは行えない。代わりに出現した単語のみの情報だけをデータに格納する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 5)\t1\n",
      "  (0, 7)\t1\n"
     ]
    }
   ],
   "source": [
    "print(new_post_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**toarray()**を使うことで、特徴ベクトルのすべての要素を表示することができる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]] , type=<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(\"%s , type=%s\" % (new_post_vec.toarray(), type(new_post_vec.toarray())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "類似度を算出するために、新しい文書と他の既存の文書の間でユークリッド距離を計算することにする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "\n",
    "def dist_raw(v1, v2):\n",
    "    delta = v1 - v2\n",
    "    return sp.linalg.norm(delta.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここで**norm()**はユークリッドノルム（ユークリッド距離）を計算している。定義したdist_raw()を用いることで、他のすべての文書に対して類似度を計算することができる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Post 0 with dist=4.00: This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "=== Post 1 with dist=1.73: Imaging databases provide storage capabilities.\n",
      "=== Post 2 with dist=2.00: Most imaging databases save images permanently.\n",
      "=== Post 3 with dist=1.41: Imaging databases store data.\n",
      "=== Post 4 with dist=5.10: Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      "\n",
      "[Result]\n",
      "Best post is 3 with dist=1.41\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# 引数にベクトルの距離を計算する関数を渡す\n",
    "def print_doc_dists(dist_func):\n",
    "    best_doc = None\n",
    "    best_dist = sys.maxsize\n",
    "    best_i = None\n",
    "\n",
    "    for i in range(0, num_samples):\n",
    "        post = posts[i]\n",
    "    \n",
    "        if post == new_post:\n",
    "            continue\n",
    "\n",
    "        post_vec = x_train.getrow(i)\n",
    "\n",
    "        d = dist_func(post_vec, new_post_vec)\n",
    "        print(\"=== Post %i with dist=%.2f: %s\" % (i, d, post))\n",
    "\n",
    "        if d < best_dist:\n",
    "            best_dist = d\n",
    "            best_i = i\n",
    "\n",
    "    print(\"\\n[Result]\\nBest post is %i with dist=%.2f\" % (best_i, best_dist))\n",
    "\n",
    "# dist_raw()を使って距離を計算するように\n",
    "print_doc_dists(dist_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "新しい文書*\"imaging databases\"*と最も似ていない文書は*文書0*である結果が出た。たしかに、これら2つの文書間には共通する単語は存在しない。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "また、新しい文書は文書1とよく似ていることが分かる。しかしながら最も類似しているものは文書3である。文書1と文書3はともに新しい文書の単語を含むが、文書1のほうが1単語多い文章から構成されている。よって文書3のほうが類似度が高くなる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "新しい文書に対しての類似度は、その2つの文書で同じであるべきだ。しかしながら結果は異なっている。ここで文書3と文書4の特徴ベクトルを出力してみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]]\n",
      "[[0 0 0 0 3 3 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train.getrow(3).toarray())\n",
    "print(x_train.getrow(4).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "単語の出現回数だけを特徴量として用いているようである。これは正規化する必要がある。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 単語の出現回数ベクトルを正規化する\n",
    "\n",
    "単語の出現回数からなるベクトルではなく、正規化したベクトルを返すようにdist_raw()を拡張してみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dist_norm(v1, v2):\n",
    "    v1_normalized = v1 / sp.linalg.norm(v1.toarray())\n",
    "    v2_normalized = v2 / sp.linalg.norm(v2.toarray())\n",
    "\n",
    "    delta = v1_normalized - v2_normalized\n",
    "    \n",
    "    return sp.linalg.norm(delta.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このdist_norm()を使って、類似度を計算してみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Post 0 with dist=1.41: This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "=== Post 1 with dist=0.86: Imaging databases provide storage capabilities.\n",
      "=== Post 2 with dist=0.92: Most imaging databases save images permanently.\n",
      "=== Post 3 with dist=0.77: Imaging databases store data.\n",
      "=== Post 4 with dist=0.77: Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      "\n",
      "[Result]\n",
      "Best post is 3 with dist=0.77\n"
     ]
    }
   ],
   "source": [
    "print_doc_dists(dist_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回の結果は文書3と文書4で同じ類似度が出ていることが分かる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 重要度の低い単語を取り除く\n",
    "\n",
    "今度は新しい文書*「imaging databases」*と文書2を比較してみることにする。新しい文書には、*most*、*safe*、*images*、*permanently*が含まれていない。こういった単語は文書内では重要度がそれぞれ異なっている。*most*は分野に関係なく様々な文章に登場してくる。こういった単語を**ストップワード（stop word）**として、処理の対象外とすべきである。ストップワードとは頻繁に使われる単語、文書の分類に貢献しない単語のことである。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizerでは、簡単にストップワードを登録することができる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df=1, stop_words='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここではenglishと指定することで、318個の単語をストップワードとして登録できる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'across', 'after', 'afterwards', 'again', 'against', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'amoungst']\n"
     ]
    }
   ],
   "source": [
    "print(sorted(vectorizer.get_stop_words())[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ストップワードを利用すると、単語リストは全部で7つ減る。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['actually', 'capabilities', 'contains', 'data', 'databases', 'images', 'imaging', 'interesting', 'learning', 'machine', 'permanently', 'post', 'provide', 'save', 'storage', 'store', 'stuff', 'toy']\n"
     ]
    }
   ],
   "source": [
    "x_train = vectorizer.fit_transform(posts)\n",
    "num_samples, num_features = x_train.shape\n",
    "print(vectorizer.get_feature_names())\n",
    "new_post_vec = vectorizer.transform([new_post])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "またストップワードを用いることで、類似度は次のようになる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Post 0 with dist=1.41: This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "=== Post 1 with dist=0.86: Imaging databases provide storage capabilities.\n",
      "=== Post 2 with dist=0.86: Most imaging databases save images permanently.\n",
      "=== Post 3 with dist=0.77: Imaging databases store data.\n",
      "=== Post 4 with dist=0.77: Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      "\n",
      "[Result]\n",
      "Best post is 3 with dist=0.77\n"
     ]
    }
   ],
   "source": [
    "print_doc_dists(dist_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文章1と文書2は同じ類似度になっていることが分かる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ステミング（stemming）\n",
    "\n",
    "意味的には同じ単語が、語形変化により異なる単語としてカウントされることである。例えば文書2では*imaging*と*images*が含まれる。これらは1つの単語としてカウントすべきである。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "単語を特定の語幹（後の語形変化に対する基本形）へ変換するためには**Natural Language Toolkit（NLTK）**を利用する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTKで*graphisc*、*imaging*、*imagination*といったものをステミングしてみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graphic\n",
      "imag\n",
      "imagin\n",
      "imagin\n"
     ]
    }
   ],
   "source": [
    "import nltk.stem\n",
    "\n",
    "s = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "print(s.stem(\"graphics\"))\n",
    "print(s.stem(\"imaging\"))\n",
    "print(s.stem(\"imagination\"))\n",
    "print(s.stem(\"imagine\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "動詞については、次のような結果になる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buy\n",
      "buy\n",
      "bought\n"
     ]
    }
   ],
   "source": [
    "print(s.stem(\"buys\"))\n",
    "print(s.stem(\"buying\"))\n",
    "print(s.stem(\"bought\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ステミングの結果は、必ずしも正しい英単語になるとは限らない。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTKのステマーを用いて、ベクトル化を拡張する\n",
    "\n",
    "文書をCountVectorizerに入れる前に、ステミングを行う必要がある。ここでは*build_analyzer()*をオーバーライドして処理を追加してみることにする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "english_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))\n",
    "\n",
    "vectorizer = StemmedCountVectorizer(min_df=1, stop_words='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここでは、次の3つのことを行っている\n",
    "\n",
    "* 前処理の段階で、文書を小文字に変換する\n",
    "* トークン化の段階で、すべての単語を抜き出す\n",
    "* それぞれの単語をステム化された単語に変換する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['actual', 'capabl', 'contain', 'data', 'databas', 'imag', 'interest', 'learn', 'machin', 'perman', 'post', 'provid', 'save', 'storag', 'store', 'stuff', 'toy']\n"
     ]
    }
   ],
   "source": [
    "x_train = vectorizer.fit_transform(posts)\n",
    "num_samples, num_features = x_train.shape\n",
    "print(vectorizer.get_feature_names())\n",
    "new_post_vec = vectorizer.transform([new_post])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ステミングを行った後で、文書をベクトル化すると、imagingとimagesが同じ単語としてカウントされていることが分かる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print_doc_dists(dist_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDFを用いる\n",
    "\n",
    "これまで考えてきた特徴量は、文書に特定の単語が何回表れるか、その回数を数えただけの単純なものであった。これは、文章中に特定の単語が多く存在すればするほど、その単語に対する重要度が高くなることを意味している。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これは**「ある単語に対して対象の文書中で出現した回数をカウントするのに加えて、その単語が*他の文書*でどれだけ出現するのかをカウントし、その回数で割る（除算する）」**という方法で対応することができる。この方法を**TF-IDF（term frequency - inverse document frequency）**で実現することができる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDFは、単語の出現頻度$TF$と逆文書頻度$IDF$の2つの指標に基づいて計算することができる。単語$t_i$の文書$d_j$における出現回数$n_{ij}$、文書$d_j$におけるすねての単語の出現回数の和$\\sum_{k}{} n_{kj}$、総文書数$|D|$、単語$t_i$を含む文書数$|{d:d \\ni t_i}|$とすると、\n",
    "\n",
    "$$\n",
    "    TF_{ij} = \\frac{n_{ij}}{\\sum_{k}{}n_{kj}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "    IDF_i = \\log \\frac{|D|}{|{d:d \\ni t_i}|}\n",
    "$$\n",
    "\n",
    "$$\n",
    "    TF-IDF = TF \\cdot IDF\n",
    "$$\n",
    "\n",
    "のように計算することができる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "import math\n",
    "\n",
    "def tf_idf(term, doc, docset):\n",
    "    tf = float(doc.count(term)) / sum(doc.count(w) for w in set(doc))\n",
    "    idf = math.log(float(len(docset)) / (len([doc for doc in docset if term in doc])))\n",
    "\n",
    "    return tf * idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3つの文書からなるデータセットを対象として、TF-IDFがどのように計算されるかを確認してみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.27031007207210955\n",
      "0.0\n",
      "0.13515503603605478\n",
      "0.3662040962227032\n"
     ]
    }
   ],
   "source": [
    "doc_a, doc_abb, doc_abc = [\"a\"], [\"a\", \"b\", \"b\"], [\"a\", \"b\", \"c\"]\n",
    "D = [doc_a, doc_abb, doc_abc]\n",
    "print(tf_idf(\"a\", doc_a, D))\n",
    "print(tf_idf(\"b\", doc_abb, D))\n",
    "print(tf_idf(\"a\", doc_abc, D))\n",
    "print(tf_idf(\"b\", doc_abc, D))\n",
    "print(tf_idf(\"c\", doc_abc, D))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "結果から\"a\"という単語は全ての文書で用いられているため、なんの意味も持たないことが分かる。また\"b\"という単語はdoc_abbという文書にとって、doc_abc文書の2倍重要であることが分かる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDFを行う機能を持ったステマ―を以下のように実装することができる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(TfidfVectorizer, self).build_analyzer()\n",
    "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))\n",
    "\n",
    "vectorizer = StemmedTfidfVectorizer(min_df=1, stop_words='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特徴ベクトルの値は、単語の出現回数ではなく、TF-IDFの値がそれぞれの単語ごとに格納されるようになった。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ここまでやってきたこと\n",
    "\n",
    "テキストデータに対して以下のような前処理を行った。\n",
    "\n",
    "1. テキストデータをトークン化する。\n",
    "2. 頻出しすぎる単語は、関連する文書を見つけるために役立たないため、取り除く。\n",
    "3. めったに使われない単語は、新しい文書でも使われる可能性が低いため、取り除いた。\n",
    "4. 残った単語についてその出現回数をカウントする。\n",
    "5. 文書全体の状況を考慮するため、単語の出現回数からTF-IDFを計算する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bag-of-wordsによるアプローチは、優れた性能を発揮するが、次のような欠点も存在している。\n",
    "\n",
    "* 単語の関連性について考慮されていない\n",
    "* 否定的な意味を正しく捉えることができていない\n",
    "* タイプミスに対応できていない"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
